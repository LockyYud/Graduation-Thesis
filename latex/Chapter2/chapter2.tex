\chapter{Cơ sở lý thuyết}
\label{chapter:related_work}
\section{Chatbot}
\label{section:chatbot}
\subsection{Định nghĩa}
\label{subsection:definition}


Trong tài liệu khoa học, chatbot thường được gọi chính thức là tác nhân hội thoại (conversational agents). Trong ngữ cảnh của khóa luận này, các thuật ngữ chatbot/tác nhân hội thoại sẽ được sử dụng thay thế cho nhau.


Nguyên tắc cơ bản của mọi chatbot là tương tác với người dùng (trong hầu hết các trường hợp) thông qua tin nhắn văn bản và hành xử như thể nó có khả năng hiểu cuộc trò chuyện và trả lời người dùng một cách phù hợp. Nguồn gốc của việc máy tính giao tiếp với con người lâu đời như chính lĩnh vực khoa học máy tính. Thật vậy, Alan Turing đã định nghĩa một bài kiểm tra đơn giản, hiện được gọi là bài kiểm tra Turing, vào năm 1950, trong đó một giám khảo là con người sẽ dự đoán xem thực thể mà họ đang giao tiếp qua tin nhắn có phải là một chương trình máy tính hay không \cite{radford2018improving}. Tuy nhiên, tham vọng của bài kiểm tra này lớn hơn nhiều so với trường hợp sử dụng thông thường của chatbot; điểm khác biệt chính là kiến thức chuyên môn của chatbot thường hẹp, trong khi bài kiểm tra Turing giả định rằng một người có thể trò chuyện về bất kỳ chủ đề nào với tác nhân. Điều này giúp ích trong việc thiết kế các tác nhân hội thoại vì chúng không cần phải có một kiến thức chuyên môn (có thể là) vô hạn mà có thể tập trung vào các chủ đề rất cụ thể, chẳng hạn như giúp người dùng đặt bàn tại một nhà hàng.


Hơn nữa, một giả định chung khác mà các nhà thiết kế chatbot thường lưu ý là người dùng thường có một mục tiêu mà họ muốn đạt được vào cuối cuộc trò chuyện khi họ bắt đầu tương tác với chatbot. Điều này sau đó ảnh hưởng đến luồng và chủ đề của cuộc trò chuyện để đạt được mục tiêu đã chọn. Các nhà phát triển có thể khai thác điều này vì các mô hình hành vi nhất định có xu hướng xuất hiện như một kết quả.


Do đó, định nghĩa về chatbot được sử dụng trong tài liệu này là một chương trình máy tính giao tiếp bằng văn bản theo cách giống con người và cung cấp dịch vụ cho người dùng nhằm hoàn thành một mục tiêu được xác định rõ ràng.


\subsection{Góc nhìn tổng quan về Chatbot}
\label{subsection:overview}


Về mặt khái niệm, một chatbot được cấu thành từ nhiều thành phần hoạt động đồng bộ nhằm đạt được một mục tiêu chung.


Khi nhận được một tin nhắn mới, bước đầu tiên là xử lý nó thông qua mô-đun nhận diện ngôn ngữ. Quá trình này có thể đơn giản như việc truy xuất một thẻ (tag) hoặc phức tạp hơn với các phương pháp thống kê. Tin nhắn mới, cùng với thông tin ngôn ngữ và các tin nhắn trước đó được lấy từ hệ thống backend, sẽ được đưa vào mô-đun phân loại ý định. Vai trò của mô-đun này là suy luận ý định mà người dùng muốn truyền đạt.


Tiếp theo, metadata của tin nhắn, ý định suy luận được, và các thông tin khác từ backend sẽ được sử dụng để xác định một hành động hoặc chuỗi hành động phù hợp. Ví dụ, chatbot có thể quyết định trả lời bằng một câu hỏi nếu ý định của người dùng chưa rõ ràng, hoặc kích hoạt lại tài khoản người dùng nếu ý định của họ là yêu cầu khôi phục tài khoản.


Cuối cùng, mô-đun xử lý hành động nhận đầu vào là hành động được xác định và thực hiện hành động đó một cách phù hợp. Việc thiết kế theo cách này là hữu ích vì một hành động có thể được thực thi theo nhiều cách khác nhau tùy thuộc vào môi trường hoạt động của chatbot. Cách thực hiện hành động có thể hoàn toàn khác biệt nếu chatbot hoạt động trên nền tảng Messenger so với trên website của một công ty.


\subsection{Phân loại ý định}
\label{subsection:intent_classification}


Khi nhận được một tin nhắn mới, tác nhân hội thoại cần có khả năng xác định mục tiêu mà người dùng đang cố gắng đạt được. Điều này thường được mô hình hóa như một bài toán phân loại đa lớp, trong đó các nhãn đại diện cho tên của các ý định khả thi từ phía người dùng. Các kỹ thuật để giải quyết vấn đề này dao động từ phương pháp trích xuất từ khóa đơn giản đến suy luận Bayes nhằm xác định yêu cầu của người dùng dựa trên nhiều tin nhắn.


Các mạng mô hình ngôn ngữ lớn (\gls{llm}) đã được chứng minh là hoạt động hiệu quả trong lĩnh vực này trước đây \cite{gao2023retrievalaugmented}. Chúng là nền tảng, xương sống của khóa luận này


\subsection{Quản lý tri thức}
\label{subsection:knowledge_management}


Một tác nhân thông minh chỉ có thể hoạt động hiệu quả trong giới hạn nếu thiếu kiến thức. Lĩnh vực cho phép máy tính xử lý kiến thức đã có những tiến bộ đáng kể trong thập niên 1980, với tên gọi kỹ thuật tri thức (knowledge engineering). Các kỹ thuật ban đầu thường sử dụng một bộ suy luận (inference engine) để xử lý các dữ kiện và suy ra kiến thức mới bằng cách sử dụng logic bậc nhất và bậc hai. Đây là một cách để suy diễn câu trả lời cho những câu hỏi không đầy đủ và thường dễ dàng được chuyển thành các lệnh gọi API.


Đối với các tác nhân đối thoại (conversational agents), kỹ thuật tri thức rất hữu ích, chẳng hạn để trả lời các câu hỏi cơ bản về các sự kiện tổng quát. Siri và Amazon Alexa sử dụng các phương pháp suy luận tri thức nội bộ để truy xuất thông tin từ web và các nguồn khác (ví dụ, khi hỏi Alexa về các chuyến tàu khởi hành từ Brussels hôm nay, hệ thống có thể kích hoạt một phép suy luận nội bộ dưới dạng train(brussels, D, today), trong đó D là một biến ẩn danh đại diện cho điểm đến).


Ngày nay, việc quản lý tri thức chủ yếu được thực hiện thông qua các lệnh gọi API và các truy vấn cơ sở dữ liệu tối ưu. Mặc dù vậy, các phương pháp đặc biệt hơn lấy cảm hứng từ các ontology có cấu trúc đồ thị đôi khi vẫn được sử dụng trong các cơ sở tri thức \cite{li2024matching}.


\subsection{Đánh giá hiệu suất}
\label{subsection:performance_assessment}


Một lĩnh vực cần cải thiện trong lĩnh vực tác nhân hội thoại là việc đánh giá hiệu suất và các chỉ số được sử dụng để định lượng chất lượng hành vi của chatbot. Trong nghiên cứu “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”, Liu và cộng sự chỉ ra rằng các chỉ số chuẩn trong lĩnh vực này, chẳng hạn như điểm số BLEU \cite{karpukhin2020dense} và ROUGE \cite{burges2005learning}, thường không tương quan với đánh giá trực quan của con người \cite{robertson2009probabilistic}.


Vấn đề cốt lõi nằm ở chỗ ngôn ngữ và sự trôi chảy vốn mang tính chủ quan, do đó rất khó để định lượng chính xác, tương tự như bất kỳ thuộc tính mang tính chủ quan nào khác của một hệ thống.


\section{Mô hình ngôn ngữ lớn}
\label{section:large_language_model}
Các mô hình ngôn ngữ tìm cách giải quyết nhiệm vụ dự đoán từ tiếp theo dựa trên một chuỗi các từ đứng trước đó \cite{zhao2023survey}. Nhiệm vụ này tạo thành cốt lõi của việc hiểu ngôn ngữ và đặt nền tảng cho các nhiệm vụ \gls{nlp} cụ thể hơn. Nghiên cứu về các mô hình ngôn ngữ bắt đầu từ những năm 1980 với sự phát triển của các mô hình ngôn ngữ thống kê (Statistical Language Models - SLM) \cite{rosenfeld2000two}. Những mô hình này dựa trên các phương pháp xác suất n-gram, được thiết kế để tính toán khả năng xuất hiện của từ tiếp theo trong một câu chưa hoàn chỉnh.


Bước tiến đáng kể tiếp theo trong các mô hình ngôn ngữ đến từ việc giới thiệu mạng Nơ-ron nhân tạo (Neural Networks) và sự phát triển của các Mô hình ngôn ngữ dựa trên mạng Nơ-ron (Neural-based Language Models - NLM). NLM mở ra cơ hội cho các nghiên cứu \gls{nlp} sâu hơn bằng cách chuyển từ các biểu diễn n-gram thưa thớt sang các biểu diễn vectơ dày đặc có kích thước thấp hơn của văn bản \cite{bengio2000neural}. Những đổi mới như Word2Vec của Mikolov, K. Chen, Corrado, và Dean \cite{mikolov2013efficient} và GloVe của Pennington, Socher, và Manning \cite{pennington2014glove} đã giúp thúc đẩy lĩnh vực này bằng cách giới thiệu các kỹ thuật hiệu quả để nắm bắt các biểu diễn ngữ nghĩa của từ, cải thiện khả năng hiểu sự tương đồng và ngữ cảnh của từ. Khi sức mạnh tính toán trở nên mạnh mẽ và dễ tiếp cận hơn, nghiên cứu về Mạng Nơ-ron tiếp tục phát triển.


Các mô hình ngôn ngữ lớn (Large Language Models - \gls{llm}), thường ám chỉ các mô hình dựa trên kiến trúc Transformer được giới thiệu bởi Vaswani, Shazeer, Parmar, và cộng sự \cite{vaswani2017attention}, đại diện cho một cột mốc khác trong NLP. Các mô hình dựa trên bộ mã hóa (Encoder) như BERT \cite{devlin2018bert} sử dụng Transformers để tạo ra các biểu diễn từ nhạy ngữ cảnh bằng cách xem xét toàn bộ ngữ cảnh của câu. Mặt khác, các mô hình dựa trên bộ giải mã (Decoder) như GPT \cite{radford2018improving}, Llama \cite{touvron2023llama}, và PaLM \cite{chowdhery2023palm} xử lý cốt lõi của các mô hình ngôn ngữ bằng cách cải thiện nhiệm vụ dự đoán từ tiếp theo dựa trên một tập hợp các từ trước đó. Những mô hình này được tinh chỉnh thêm để tạo ra các chuỗi văn bản mạch lạc khi nhận được một gợi ý.


Các mô hình Seq2Seq như BART \cite{lewis2019bart} và T5 \cite{raffel2020exploring} bao gồm cả thành phần mã hóa và giải mã. Khác với các mô hình chỉ sử dụng giải mã, vốn xử lý đầu vào theo hướng đơn chiều, các mô hình encode-decode mã hóa đầu vào bằng các cơ chế chú ý hai chiều để hiểu rõ ngữ cảnh. Sau đó, biểu diễn vectơ được đưa vào bộ giải mã để tạo văn bản, xử lý các nhiệm vụ như tóm tắt, dịch thuật, và nhiều nhiệm vụ tạo ngôn ngữ phức tạp khác.


Mặc dù đã đạt được nhiều tiến bộ, các \gls{llm} tiêu tốn nhiều tài nguyên và đòi hỏi sức mạnh tính toán lớn để huấn luyện, điều này làm cho việc tích hợp và cập nhật dữ liệu thời gian thực trở nên không khả thi. Hơn nữa, vì thông tin được chứa trong các tham số tĩnh, các \gls{llm} không đáng tin cậy trong việc duy trì độ chính xác về mặt thông tin. Những hạn chế này đã dẫn đến các nghiên cứu sâu hơn, bao gồm RAG, vốn kết hợp thông tin liên quan một cách hiệu quả trong quá trình tạo nội dung \cite{gao2023retrievalaugmented}. Mục tiêu chính của \gls{rag} là cải thiện phản hồi của \gls{llm}, đảm bảo chúng vừa chính xác về mặt thông tin vừa có căn cứ bằng cách tận dụng dữ liệu được chọn lọc.


\section{Truy xuất thông tin}
\label{section:information_retrieval}


Trước khi được sử dụng để tăng cường \gls{llm}s, Truy xuất thông tin (Information Retrieval - IR) đã được giới thiệu nhằm đơn giản hóa việc điều hướng các cơ sở dữ liệu lớn bằng cách cho phép người dùng truy cập thông tin thông qua các truy vấn văn bản. Nhiệm vụ \gls{nlp} này đã được sử dụng rộng rãi trong các chương trình như công cụ tìm kiếm và hệ thống gợi ý \cite{li2024matching}. Các tiến bộ trong IR song hành với sự phát triển của các mô hình ngôn ngữ, với các kỹ thuật ban đầu dựa vào đếm từ thưa thớt và n-grams. Các phương pháp nổi bật bao gồm Boolean Retrieval \cite{salton1983extended}, Term Frequency-Inverse Document Frequency (TF-IDF) \cite{luhn1957statistical}, và BM25 \cite{robertson2009probabilistic}, những phương pháp này cố gắng trích xuất các tài liệu liên quan bằng cách khớp các từ trong truy vấn.


Với sự trỗi dậy của mạng nơ-ron, IR đã phát triển song song với các mô hình ngôn ngữ và chuyển hướng sang các biểu diễn vectơ dày đặc. Hiện nay, kỹ thuật truy xuất phổ biến nhất cho các nhiệm vụ \gls{nlp} khác nhau là Dense Passage Retrieval (DPR) được giới thiệu bởi Karpukhin, O˘guz, S. Min, và cộng sự \cite{karpukhin2020dense}, sử dụng các biểu diễn vectơ dày đặc để truy xuất dữ liệu. DPR là một mô hình sử dụng hai bộ mã hóa: một bộ mã hóa các tài liệu để lưu trữ, và một bộ mã hóa các câu hỏi để liên kết với các vectơ tài liệu. Cách tiếp cận này đã chứng minh sự cải thiện đáng kể so với các biểu diễn vectơ thưa truyền thống.


Nhiều nghiên cứu đã tập trung vào việc tinh chỉnh kết quả truy xuất bằng cách xếp hạng dữ liệu thu thập được để mang lại kết quả chính xác hơn. RankNet của Burges, Shaked, Renshaw, và cộng sự \cite{burges2005learning} và ListNet của Cao, Qin, T.-Y. Liu, và cộng sự \cite{cao2007learning} là hai kỹ thuật nền tảng được thiết kế để sắp xếp lại danh sách thông tin đã truy xuất nhằm cải thiện mức độ liên quan. RankNet sử dụng phương pháp pairwise, trong đó một truy vấn và hai tài liệu được đưa vào để xác định tài liệu nào liên quan hơn trong cặp. Ngược lại, ListNet sử dụng phương pháp listwise và xử lý toàn bộ danh sách các tài liệu đã truy xuất cùng với truy vấn để đánh giá mức độ liên quan của từng tài liệu và sắp xếp dữ liệu tương ứng.


Những tiến bộ trong mạng Nơ-ron sâu (Deep Neural Network) và \gls{llm}s đã cho phép triển khai các kỹ thuật cross-encoding để tái xếp hạng tài liệu, như được chứng minh bởi Nogueira và Cho \cite{nogueira2019passage}. Trong phương pháp này, một mô hình tái xếp hạng với cơ chế cross-encoding xử lý đồng thời truy vấn và các tài liệu để đánh giá chính xác hơn, so với việc chỉ sử dụng điểm tương đồng của các vectơ dày đặc độc lập. Phương pháp này đã được chứng minh là cải thiện đáng kể độ chính xác của việc truy xuất nhờ khả năng phân tích kỹ lưỡng mối quan hệ giữa truy vấn và tài liệu. Tuy nhiên, các mô hình tái xếp hạng đòi hỏi nhiều tài nguyên tính toán, đặc biệt khi phải đánh giá tất cả các tài liệu trong cơ sở dữ liệu lớn. Do đó, các mô hình tái xếp hạng thường được sử dụng để tinh chỉnh kết quả truy xuất sau khi sử dụng DPR hoặc các phương pháp khác để lọc ban đầu.


\section{Tạo tăng cường truy xuất (RAG)}
\label{section:retrieval_augmented_generation}
Tạo tăng cường truy xuất (Retrieval-Augmented Generation - RAG) đã nổi lên như một phương pháp quan trọng để cải thiện khả năng của các mô hình ngôn ngữ bằng cách kết hợp thông tin bên ngoài vào quá trình mô hình hóa sinh văn bản. Phương pháp này gồm 3 giai đoạn chính: Truy xuất (Retrieval), kết hợp truy xuất (Fusion Retrieval), Tạo sinh (Generation). Nhưng trong phạm vi bài nghiên cứu này sẽ chỉ tập trung vào việc cải thiện khả năng truy xuất, suy luận.


\subsection{RAG truyền thống dựa trên truy xuất cơ sở dữ liệu vector}
\label{subsection:traditional_rag}


Nhiều kỹ thuật đã được giới thiệu nhằm tích hợp dữ liệu văn bản được truy xuất và các mô hình ngôn ngữ, một số kỹ thuật trong đó xuất hiện trước khi thuật ngữ RAG được giới thiệu. Ví dụ, bài báo Retrieval Augmented Language Model Pre-Training (REALM) của Guu, Lee, Tung, và cộng sự \cite{guu2020realm} đánh dấu một trong những kỹ thuật sớm nhất tích hợp việc truy xuất tri thức trong quá trình huấn luyện một mô hình ngôn ngữ. Cách tiếp cận này chia nhiệm vụ thành hai phần: một bộ truy xuất tri thức nơ-ron để truy xuất tài liệu và một bộ mã hóa tăng cường tri thức để diễn giải các tài liệu được truy xuất. Dựa trên kiến trúc BERT, bộ truy xuất tri thức nơ-ron mã hóa các truy vấn và tài liệu để xác định và truy xuất tài liệu phù hợp nhất cho từng truy vấn. Sau khi truy xuất, bộ mã hóa tăng cường tri thức sẽ lấy đầu vào là sự kết hợp giữa truy vấn gốc và tài liệu đã truy xuất. Đầu vào kết hợp này được xử lý thông qua kiến trúc Transformer, cho phép sự tương tác chéo phong phú giữa truy vấn và tài liệu. Không giống như nhiều mô hình sinh, REALM tập trung vào các nhiệm vụ trích xuất, huấn luyện hệ thống một cách từ đầu tới cuối để tận dụng tri thức truy xuất và khả năng hiểu ngôn ngữ một cách liền mạch.


Thuật ngữ \gls{rag} lần đầu được giới thiệu bởi P. Lewis, Perez, Piktus, và cộng sự \cite{lewis2020retrieval}. \gls{rag} đánh dấu một bước tiến đáng kể trong \gls{nlp} bằng cách tích hợp bộ truy xuất thần kinh và bộ sinh Seq2Seq để cải thiện các nhiệm vụ đòi hỏi nhiều tri thức. Không giống như REALM, tập trung vào hỏi đáp trích xuất (extractive QA) sử dụng một mô hình duy nhất cho việc truy xuất và mã hóa, \gls{rag} giới thiệu một kiến trúc khác biệt, kết hợp một DPR để truy xuất tài liệu và một BART để tạo sinh chuỗi. Ngoài ra, các mô hình truy xuất và sinh được huấn luyện đầu-cuối trong khi giữ nguyên các vector embedding của mỗi tài liệu, bởi việc cập nhật embedding trong quá trình huấn luyện vừa tốn kém về mặt tính toán vừa không cần thiết.




Sau đó, nhiều thuật toán khác đã xuất hiện và tích hợp các phương pháp truy xuất hoặc mô hình sinh khác nhau để cải thiện kỹ thuật RAG. Ví dụ, trong một nghiên cứu của Izacard và Grave \cite{izacard2020leveraging}, cách xử lý dữ liệu truy xuất khác biệt. Mô hình tạo sinh câu trả lời của thuật toán được thiết kế bằng mô hình Fusion-in-Decoder (FiD), có khả năng mở rộng để xử lý số lượng dữ liệu văn bản lớn hơn. Mô hình này mã hóa sự kết hợp giữa câu hỏi và từng tài liệu riêng biệt, sau đó tổng hợp các mã hóa vector này trong bộ giải mã. Hệ thống tạo ra các câu trả lời phong phú về ngữ cảnh thông qua các cơ chế attention được áp dụng trên các mã hóa kết hợp.


Retrieval-Enhanced Transformer (RETRO) của Borgeaud, Mensch, và cộng sự \cite{borgeaud2022improving} là một mô hình ngôn ngữ khác được đề xuất nhằm cải thiện các kỹ thuật \gls{rag} và DPR. RETRO cho phép việc truy xuất lặp lại trong suốt quá trình sinh, không chỉ dựa trên gợi ý ban đầu như \gls{rag} hay FiD mà liên tục khi dữ liệu truy xuất được mở rộng. Khả năng truy xuất liên tục này cho phép RETRO thích ứng động với ngữ cảnh được thu thập từ văn bản. RETRO cũng được huấn luyện từ đầu, sử dụng kiến trúc Transformer và một cơ chế cross-attention theo từng đoạn; tuy nhiên, embedding được tính toán bằng cách sử dụng mô hình BERT cố định. Các tài liệu được chia thành nhiều phân đoạn, mã hóa và lưu trữ trong cơ sở dữ liệu vector để truy xuất nhanh hơn. RETRO hoạt động tương đương với GPT-3 và Jurassic-1 nhưng sử dụng ít hơn 25 lần số lượng tham số.


Nhiều bài báo khác đã được xuất bản để nghiên cứu sự phù hợp của việc tích hợp nhiều nguồn và loại dữ liệu vào hệ thống RAG. Bài báo "Unified representations of structured and unstructured knowledge for open-domain question answering" (UniK-QA) của Oguz, X. Chen, Karpukhin, và cộng sự \cite{oguz2020unikqa} phân tích sự liên quan của việc kết hợp các loại dữ liệu khác nhau để tăng cường các mô hình ngôn ngữ. Mô hình FiD, được giới thiệu bởi Izacard và Grave \cite{izacard2020leveraging}, đã được sử dụng cho \gls{rag} với một cơ sở dữ liệu kết hợp dữ liệu có cấu trúc, không cấu trúc và bán cấu trúc, kết hợp dữ liệu văn bản và cơ sở tri thức.


Tất cả các loại dữ liệu đều được biến đổi theo cách heuristic thành văn bản, và quá trình truy xuất được thực hiện bằng phương pháp DPR. Y. Li, Peng, Shen, và cộng sự \cite{li2021knowledge} cũng tích hợp các loại dữ liệu và nguồn khác nhau để truy xuất bằng cách giới thiệu mô hình ngôn ngữ PLUG. Mô hình này được huấn luyện sử dụng nhiều nguồn tri thức khác nhau, từ văn bản đến dữ liệu từ đồ thị tri thức (KG). Quá trình truy xuất khá đơn giản, dựa trên các truy vấn tìm kiếm và khớp từ khóa để tìm dữ liệu liên quan. Sau đó, dữ liệu được biến đổi thành văn bản và lọc qua hai giai đoạn: xếp hạng thống kê và xếp hạng ngữ nghĩa. Xếp hạng thống kê sử dụng TF-IDF để tạo một tập dữ liệu ban đầu. Sau đó, tập này được lọc thêm bằng xếp hạng ngữ nghĩa dựa trên điểm tương đồng của sentence-BERT và một ngưỡng được đặt trước.


Không giống như các nghiên cứu khác tinh chỉnh các mô hình để tạo câu trả lời từ các tài liệu được truy xuất và sử dụng cơ chế cross-attention giữa các tài liệu và đầu vào, Re-plug của Weijia, Sewon, Michihiro, và cộng sự \cite{weijia2023replug} coi mô hình ngôn ngữ lớn sinh (\gls{llm}) như một hộp đen. Mỗi tài liệu được truy xuất từ hệ thống \gls{rag} được sử dụng riêng để tăng cường \gls{llm} và tạo ra xác suất cho các token đầu ra, sau đó các xác suất này được tổng hợp trên các tài liệu để tính toán xác suất token cuối cùng. Kỹ thuật này ngăn chặn việc cắt bớt các tài liệu được thêm vào nếu chúng vượt quá kích thước ngữ cảnh của \gls{llm}.


\subsection{Tạo sinh tăng cường truy xuất dựa trên đồ thị}
\label{subsection:graph_based_rag}


GraphRAG (Tạo sinh tăng cường truy xuất dựa trên đồ thị) \cite{cao2007learning,nogueira2019passage,guu2020realm,izacard2020leveraging,borgeaud2022improving,oguz2020unikqa} là một phương pháp mới tận dụng đồ thị tri thức để cải thiện hiệu suất của các tác vụ \gls{nlp} như hệ thống hỏi đáp. Bằng cách tích hợp \gls{kg}s với các kỹ thuật RAG, GraphRAG cho phép tạo ra các phản hồi chính xác và có ngữ cảnh hơn dựa trên thông tin có cấu trúc được trích xuất từ các tài liệu tài chính. Tuy nhiên, GraphRAG thường hoạt động kém hiệu quả trong các tác vụ hỏi đáp mang tính trừu tượng hoặc khi câu hỏi không đề cập rõ ràng đến thực thể cụ thể nào.


Nhiều nghiên cứu đã khám phá các cách để trích xuất thông tin từ đồ thị tri thức và cơ sở dữ liệu đồ thị, tận dụng định dạng có cấu trúc của chúng. Một trong những hệ thống hỏi đáp dựa trên đồ thị tri thức sớm nhất thực hiện một quá trình nhiều giai đoạn để liên kết các câu hỏi của người dùng với các thực thể trong đồ thị và trích xuất câu trả lời. Đầu tiên, các thực thể trong câu hỏi của người dùng được trích xuất và liên kết với các nút tương ứng trong đồ thị tri thức. Sau đó, việc phát hiện mối quan hệ được thực hiện để tìm ra cạnh thích hợp trong đồ thị dẫn đến câu trả lời chính xác. Ví dụ, Berant, Chou, Frostig và Liang \cite{berant2013semantic} đã đề xuất một phương pháp phân tích ngữ nghĩa để chuyển đổi các câu hỏi văn bản thành các dạng logic hoặc truy vấn có thể thực thi, từ đó trích xuất câu trả lời từ cơ sở dữ liệu đồ thị. Các kỹ thuật khác, chẳng hạn như những kỹ thuật được giới thiệu bởi Z. Wang, Ng, Nallapati và Xiang \cite{wang2021retrieval}, sử dụng phương pháp truy xuất, tái xếp hạng và học đa nhiệm để cải thiện độ chính xác trong việc phát hiện thực thể, liên kết và xếp hạng.


Các tiến bộ gần đây nhằm loại bỏ các quy trình nhiều bước của việc truy xuất dữ liệu dựa trên đồ thị truyền thống để tránh sự tích lũy lỗi. Một trong những cách tiếp cận như vậy là phương pháp Direct Fact Retrieval - DiFaR do Baek, Aji, Lehmann và Hwang \cite{baek2023direct} đề xuất. Phương pháp này trích xuất tất cả các bộ ba từ cơ sở dữ liệu và mã hóa chúng thành một embedding vector dày đặc tương tự như kỹ thuật DPR. Phương pháp này vì vậy lưu trữ các mối quan hệ logic trong cơ sở dữ liệu vector thay vì chỉ lưu trữ dữ liệu văn bản. Nghiên cứu cũng sử dụng một mô hình tái xếp hạng (re-ranker) nhận đầu vào là câu hỏi và tài liệu, sau đó đưa ra một điểm xếp hạng chỉ ra tính hữu ích của tài liệu trong việc trả lời câu hỏi. Các bộ ba hàng đầu K được truy xuất từ cơ sở dữ liệu vector, và chỉ có các bộ ba này được đưa qua mô hình tái xếp hạng để sắp xếp lại mức độ liên quan của chúng với câu hỏi.


Y. Wang, Lipka, Rossi và cộng sự \cite{wang2024knowledge} cũng tận dụng tính có cấu trúc của cơ sở dữ liệu đồ thị trong quá trình truy xuất của một hệ thống RAG. Bài báo này xây dựng một đồ thị tri thức sử dụng một tập hợp các tài liệu và cấu trúc dữ liệu khác. Sau đó, một phương pháp duyệt đồ thị dựa trên mô hình ngôn ngữ lớn (\gls{llm}) được thiết kế để truy xuất thông tin liên quan từ cơ sở dữ liệu đồ thị. Ban đầu, TF-IDF được sử dụng để so sánh nội dung của các nút với truy vấn đã cho. Nút có điểm tương đồng cao nhất được trích xuất và coi là điểm khởi đầu cho việc duyệt đồ thị. Sau đó, chức năng này xếp hạng và điều hướng qua các nút lân cận để thu thập thông tin quý giá cho việc gợi ý \gls{llm}. Một mô hình \gls{llm} đã được tinh chỉnh được sử dụng để xếp hạng các nút lân cận, trích xuất nút có dữ liệu liên quan nhất và xác định các bước duyệt tiếp theo. \gls{llm} được huấn luyện để xem xét cả câu hỏi và các nút đã duyệt trước đó để chọn nút hứa hẹn nhất tiếp theo, đảm bảo rằng việc hiểu biết dữ liệu đã được truy xuất tích lũy sẽ thông báo cho từng bước duyệt. Kỹ thuật này làm tăng khả năng chọn các nút với ít sự lặp lại thông tin và chất lượng cao hơn như một bổ sung vào thông tin đã thu thập. So với các kỹ thuật embedding vector, thường dựa vào dữ liệu không liên kết với các biểu diễn vector cô đặc, duyệt đồ thị nắm bắt các kết nối logic tiềm ẩn trong \gls{kg}s, tận dụng các thuộc tính cấu trúc thường bị mất trong không gian vector. Điều này làm cho các phương pháp dựa trên đồ thị đặc biệt mạnh mẽ trong việc trả lời câu hỏi, đặc biệt là những câu hỏi cần đến lý luận quan hệ và nhận thức về ngữ cảnh.


Tiếp đó Darren Edge, Ha Trinh và cộng sự \cite{microsoftGraphRAG} cũng đã tận dụng tính có cấu trúc của cơ sở dữ liệu đồ thị dựa trên việc tóm tắt toàn cục từ đồ thị tri thức được tạo bởi \gls{llm}. Bài báo này xây dựng một phương pháp sử dụng một \gls{llm} để xây dựng chỉ mục văn bản dựa trên đồ thị qua hai giai đoạn: giai đoạn đầu tiên là tạo ra một đồ thị tri thức thực thể từ các tài liệu nguồn, giai đoạn sau đó là tiền tạo các bản tóm tắt cộng đồng cho tất cả các nhóm thực thể có mối liên hệ chặt chẽ. Với một truy vấn sẽ được sử dụng để tìm kiếm trong đồ thị tri thức nhằm truy xuất các nút (thực thể) và cạnh (quan hệ) liên quan đến truy vấn. Một đồ thị con, bao gồm các nút và cạnh liên quan này, được trích xuất từ toàn bộ \gls{kg} để cung cấp ngữ cảnh. Đồ thị con này sau đó được tích hợp với tri thức nội tại của mô hình ngôn ngữ bằng cách mã hóa cấu trúc đồ thị thành các biểu diễn nhúng (embeddings) mà mô hình có thể hiểu được. Các bản tóm tắt do \gls{llm} tạo ra từ các mô tả đồ thị con này cung cấp phạm vi bao phủ đầy đủ của chỉ mục đồ thị cơ bản và các tài liệu đầu vào mà nó đại diện. Việc tóm tắt tập trung vào truy vấn của toàn bộ tập dữ liệu sau đó được thực hiện bằng cách sử dụng cách tiếp cận map-reduce: đầu tiên sử dụng từng bản tóm tắt đồ thị con để trả lời truy vấn một cách độc lập và song song, sau đó tóm tắt tất cả các câu trả lời từng phần liên quan thành một câu trả lời toàn cục cuối cùng.


\subsection{RAG kết hợp đồ thị tri thức và văn bản}
\label{subsection:graph_text_rag}
Đây là một đề tài nghiên cứu được chú trọng trong thời gian gần đây, tập trung vào việc nâng cao khả năng của các nhiệm vụ \gls{nlp} bằng cách tối ưu khả năng của truy xuất thông tin dựa trên cơ sở dữ liệu vector. Tuy nhiên khả năng khi áp dụng của phương pháp này đối với các văn bản dài vẫn còn nhiều hạn chế khi độ chính xác trong việc truy xuất các phản hồi liên quan vẫn là một thách thức. Trong khi đó, các phương pháp GraphRAG bằng các tích hợp đồ thị tri thức với các kỹ thuật \gls{rag} đã cho phép tạo ra các phản hồi chính xác hơn và có nhận thức về ngữ cảnh dựa trên thông tin có cấu trúc được trích xuất từ các tài liệu chính. Nhưng thường hoạt động kém trong các nhiệm vụ hỏi đáp trừu tượng hoặc khi không có thực thể rõ ràng nào được đề cập trong câu hỏi.


Từ những ưu điểm và nhược điểm của cả hai phương pháp truy xuất phía trên, nhiều nghiên cứu nổi về \gls{rag} gần đây đã đề xuất nhiều phương án kết hợp cả hai phương pháp nhằm tận dụng những khả năng và giảm bớt hạn chế của cả hai phương pháp. Các nghiên cứu tiêu biểu gần đây có thể kể đến HybridRAG được đề xuất bởi Bhaskarjit Sarmah, Benika Hall, và đồng nghiệp \cite{HybridRAG2024}. Cụ thể, phương pháp này giải quyết các hạn chế của VectorRAG (dựa trên cơ sở dữ liệu vector) và GraphRAG (sử dụng đồ thị tri thức). Với VectorRAG có thế mạnh trong việc xử lý văn bản không cấu trúc với quy mô lớn thông qua biểu diễn vector. Tuy nhiên, nó thường gặp khó khăn trong việc hiểu mối quan hệ giữa các thực thể. Còn trong khi đó, GraphRAG mang lại hiệu quả trong việc lập luận dựa trên dữ liệu có cấu trúc nhưng hạn chế về khả năng mở rộng và yêu cầu mối quan hệ được xác định trước giữa các thực thể.
HybridRAG việc tận dụng cả hai phương pháp bằng cách kết hợp các bộ mã hóa: bộ mã hóa đồ thị tri thức để lý giải mối quan hệ phức tạp giữa các thực thể và bộ mã hóa vector để trích xuất thông tin từ văn bản tự do. Kết quả sau đó được xử lý bởi một bộ giải mã lai, tạo ra các phản hồi chính xác và phù hợp với ngữ cảnh. Cách tiếp cận này đã cho thấy hiệu quả vượt trội trong các bài toán như trích xuất thực thể, phát hiện mối quan hệ, và hỏi đáp, đặc biệt trong các lĩnh vực như tài chính hoặc các tài liệu phức tạp.


Jiashuo Sun21, Chengjin Xu và cộng sự \cite{tog2_2024} đã đề xuất Think-on-Graph 2.0 mang đến một cải tiến đáng kể đối với các mô hình ngôn ngữ lớn (LLMs) bằng cách kết hợp khả năng suy luận với truy xuất có sự hướng dẫn từ tri thức. Mô hình này xây dựng trên phương pháp Think-on-Graph \cite{tog_2024} ban đầu, kết hợp việc tích hợp sâu hơn với các nguồn tri thức bên ngoài, đặc biệt là đồ thị tri thức (KGs), nhằm nâng cao độ chính xác và độ tin cậy của các phản hồi.
Điểm đổi mới chính trong khuôn khổ này là việc tăng cường khả năng suy luận của \gls{llm} với một hệ thống truy xuất hiệu quả. Bằng cách khai thác dữ liệu liên quan từ một đồ thị tri thức, mô hình không chỉ tận dụng các mẫu ngôn ngữ mà còn làm phong phú thêm khả năng suy luận bằng thông tin có cấu trúc và chính xác. Quá trình này hỗ trợ các nhiệm vụ suy luận nhiều bước và giải quyết các lỗ hổng tri thức trong thời gian thực bằng cách truy xuất thông tin giúp hình thành các phản hồi mạch lạc và chính xác về ngữ cảnh. Điều này đặc biệt hữu ích đối với các câu hỏi phức tạp, yêu cầu nhiều tri thức, nơi mà các mô hình ngôn ngữ truyền thống có thể gặp khó khăn nếu không có một nền tảng tri thức bên ngoài đáng tin cậy.
Ngoài ra, Think-on-Graph 2.0 cũng chú trọng vào khả năng giải thích, cung cấp cái nhìn về cách thức ra quyết định của mô hình dựa trên tri thức đã được truy xuất. Sự minh bạch này rất quan trọng đối với các nhiệm vụ cần giải thích hoặc biện minh cho các câu trả lời, giúp phương pháp này trở nên mạnh mẽ hơn trong các ứng dụng như chăm sóc sức khỏe, phân tích pháp lý và nghiên cứu khoa học, nơi độ chính xác và khả năng giải thích là yếu tố quan trọng.

